# Production-Scale Recommendation System
## Complete System Flow & Architecture Documentation

**Generated:** February 11, 2026
**System Scale:** Billions of requests/day â€¢ 10M+ items â€¢ 100M+ users

---

## Table of Contents

1. [System Overview](#1-system-overview)
2. [Data Pipeline Flow](#2-data-pipeline-flow)
3. [Feature Engineering Pipeline](#3-feature-engineering-pipeline)
4. [Embedding Generation](#4-embedding-generation)
5. [Two-Stage Retrieval Architecture](#5-two-stage-retrieval-architecture)
6. [Model Training & Evaluation](#6-model-training--evaluation)
7. [Production Serving](#7-production-serving)
8. [Monitoring & Observability](#8-monitoring--observability)
9. [End-to-End Request Flow](#9-end-to-end-request-flow)
10. [Key Design Decisions](#10-key-design-decisions)

---

## 1. System Overview

This is a **production-grade recommendation system** designed to handle billions of requests daily, similar to systems at Roblox, Meta, or Google. The architecture balances **user experience** (relevant recommendations) with **revenue optimization** (effective monetization).

### Key Capabilities

| Component | Description | Scale |
|-----------|-------------|-------|
| **Throughput** | Handles 10K+ queries per second | Billions of requests/day |
| **Latency** | Sub-100ms p99 latency | < 50ms typical |
| **Catalog Size** | Millions of items | 10M+ items indexed |
| **Users** | Millions of active users | 100M+ daily active |
| **Model Complexity** | Transformer-based CTR prediction | BERT + LightGBM ensemble |
| **Training Data** | 100TB+ interaction data | Daily retraining |

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Request (10K QPS)                            â”‚
â”‚          Context: user_id, device, location, timestamp               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Feature Store (Feast)                             â”‚
â”‚  â€¢ User Features: demographics, behavior (Redis < 5ms)              â”‚
â”‚  â€¢ Item Features: metadata, popularity, quality                     â”‚
â”‚  â€¢ Context Features: time, device, placement                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STAGE 1: Candidate Generation (20ms)                        â”‚
â”‚  â€¢ Two-Tower Embeddings: UserÃ—Item similarity                       â”‚
â”‚  â€¢ FAISS ANN Search: 10M items â†’ 500 candidates                     â”‚
â”‚  â€¢ Output: Top 500 candidate items                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STAGE 2: Ranking & Scoring (15ms)                           â”‚
â”‚  â€¢ LightGBM Ranker (100+ features)                                  â”‚
â”‚  â€¢ Multi-objective Score: CTR + CVR + Revenue                       â”‚
â”‚  â€¢ Output: Ranked top 50 items with scores                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STAGE 3: Business Logic (5ms)                               â”‚
â”‚  â€¢ Diversity: Max N items per category                              â”‚
â”‚  â€¢ Freshness: Boost new content                                     â”‚
â”‚  â€¢ Deduplication: Remove recently shown items                       â”‚
â”‚  â€¢ Safety: Apply content filters                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Response (Top-K Items)                             â”‚
â”‚  Total Latency: < 50ms p99 | Revenue: optimized | UX: preserved    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Data Pipeline Flow

The data pipeline processes raw user-item interactions from various sources and transforms them into clean, validated datasets ready for model training.

### 2.1 Data Sources

| Source | Type | Volume | Update Frequency |
|--------|------|--------|-----------------|
| **User Interactions** | Event logs (S3/Parquet) | Billions/day | Real-time streaming |
| **User Profiles** | Database snapshot | 100M records | Daily batch |
| **Item Metadata** | Database + CMS | 10M items | Hourly batch |
| **Context Data** | Real-time API | N/A | Per request |

### 2.2 Data Validation Pipeline

```python
# src/data_pipeline/data_loader.py

class DataValidator:
    """
    Data quality validation with comprehensive checks
    """

    @staticmethod
    def validate_interactions(df: DataFrame) -> Tuple[DataFrame, Dict]:
        """
        Multi-step validation process:

        1. Schema Enforcement
           - Validate data types
           - Ensure required fields present

        2. Quality Checks
           - Remove nulls in critical fields
           - Deduplicate interactions
           - Filter invalid timestamps
           - Validate event types

        3. Anomaly Detection
           - Detect volume spikes
           - Identify bot behavior
           - Flag outliers

        4. Metrics Tracking
           - Data quality rate: clean/total
           - Alert if quality < 95%
        """
```

**Key Validation Steps:**

1. **Remove Null Values**
   - Critical fields: user_id, item_id, timestamp
   - Optional fields: allowed to be null

2. **Deduplicate**
   - Remove duplicate (user_id, item_id, timestamp) tuples
   - Prevents double-counting

3. **Timestamp Validation**
   - Reject future timestamps
   - Filter data older than 2 years
   - Ensure timezone consistency

4. **Event Type Validation**
   - Allowed: view, click, add_to_cart, purchase, favorite
   - Reject invalid event types

5. **Anomaly Detection**
   - Daily volume: Alert if spike > 3 std dev
   - Bot detection: Flag users with > 1000 events/day
   - Statistical outliers: Identify and investigate

### 2.3 Train/Test Split Strategy

**Time-based split** (CRITICAL for recommendation systems):

```
Training Data     â”‚  Validation Data  â”‚  Test Data
T-90 to T-14      â”‚  T-14 to T-7      â”‚  T-7 to T-0
(76 days)         â”‚  (7 days)         â”‚  (7 days)
```

**Why time-based?**
- âœ… **No data leakage:** No future information in training
- âœ… **Realistic evaluation:** Simulates production scenario
- âœ… **Temporal patterns:** Accounts for seasonality and trends
- âŒ **Random split:** Can inflate metrics by 10-20% (wrong!)

**Implementation:**

```python
def create_train_test_split(
    df: DataFrame,
    test_days: int = 7,
    val_days: int = 7
) -> Tuple[DataFrame, DataFrame, DataFrame]:
    """
    Time-based split preventing data leakage
    """
    max_date = df.agg(F.max("timestamp")).collect()[0][0]

    test_start = F.date_sub(F.lit(max_date), test_days)
    val_start = F.date_sub(F.lit(max_date), test_days + val_days)

    train_df = df.filter(F.col("timestamp") < val_start)
    val_df = df.filter(
        (F.col("timestamp") >= val_start) &
        (F.col("timestamp") < test_start)
    )
    test_df = df.filter(F.col("timestamp") >= test_start)

    return train_df, val_df, test_df
```

---

## 3. Feature Engineering Pipeline

Feature engineering transforms raw data into meaningful signals. This is **the most important step** in building effective recommendation systems.

### 3.1 User Features

| Feature Category | Examples | Purpose | Computation |
|------------------|----------|---------|-------------|
| **Demographics** | Age, gender, location | Broad personalization | Profile lookup |
| **Behavior Stats** | Total interactions, CTR, avg session time | Engagement level | Aggregate metrics |
| **Preferences** | Favorite categories, brands, price range | Content affinity | Top-K aggregation |
| **Recency** | Days since last visit, last purchase | User lifecycle stage | Date difference |
| **Purchase History** | Conversion rate, avg order value, LTV | Revenue optimization | Transaction analysis |
| **Sequential** | Last 50 items interacted with | Temporal patterns | Ordered history |

**Example: User Statistics**

```python
user_stats = interactions_df.groupBy("user_id").agg(
    F.count("*").alias("total_interactions"),
    F.countDistinct("item_id").alias("unique_items"),

    # Event type distribution
    F.sum(F.when(F.col("event_type") == "view", 1)).alias("view_count"),
    F.sum(F.when(F.col("event_type") == "click", 1)).alias("click_count"),
    F.sum(F.when(F.col("event_type") == "purchase", 1)).alias("purchase_count"),

    # Recency
    F.max("timestamp").alias("last_interaction"),
    F.datediff(F.current_timestamp(), F.max("timestamp")).alias("recency_days"),

    # Conversion metrics
    (F.sum(F.when(F.col("event_type") == "purchase", 1)) /
     (F.sum(F.when(F.col("event_type") == "click", 1)) + 1)).alias("conversion_rate")
)
```

### 3.2 Item Features

| Feature Category | Examples | Purpose | Update Frequency |
|------------------|----------|---------|-----------------|
| **Content** | Title, description, category, tags | Content-based filtering | On item creation |
| **Popularity** | Total views, CTR, conversion rate | Trending identification | Hourly |
| **Quality** | Average rating, # reviews | Quality filtering | Daily |
| **Temporal** | Days since creation, trending score | Freshness boost | Hourly |
| **Text Embeddings** | BERT/sentence-transformers vectors | Semantic similarity | On update |
| **Metadata** | Price, brand, availability | Business rules | Real-time |

**Example: Item Popularity with Trending Score**

```python
# Overall popularity
item_stats = interactions_df.groupBy("item_id").agg(
    F.count("*").alias("total_interactions"),
    F.sum(F.when(F.col("event_type") == "view", 1)).alias("views"),
    F.sum(F.when(F.col("event_type") == "click", 1)).alias("clicks"),
    F.sum(F.when(F.col("event_type") == "purchase", 1)).alias("purchases"),
    F.countDistinct("user_id").alias("unique_users")
)

# Recent popularity (last 30 days)
recent_cutoff = F.date_sub(F.current_timestamp(), 30)
recent_stats = interactions_df.filter(
    F.col("timestamp") >= recent_cutoff
).groupBy("item_id").agg(
    F.count("*").alias("recent_interactions")
)

# Trending score: recent vs total
item_features = item_stats.join(recent_stats, "item_id", "left").fillna(0)
item_features = item_features.withColumn(
    "trending_score",
    F.col("recent_interactions") / (F.col("total_interactions") + 1)
)
```

### 3.3 Contextual Features

Context features capture the **situation** in which recommendations are requested:

**Temporal Features (with Cyclical Encoding):**

```python
# Why cyclical encoding?
# Hour 23 should be close to Hour 0, not far away!

df = df.withColumn(
    "hour_sin",
    F.sin(2 * np.pi * F.col("hour_of_day") / 24)
).withColumn(
    "hour_cos",
    F.cos(2 * np.pi * F.col("hour_of_day") / 24)
).withColumn(
    "day_sin",
    F.sin(2 * np.pi * F.col("day_of_week") / 7)
).withColumn(
    "day_cos",
    F.cos(2 * np.pi * F.col("day_of_week") / 7)
)
```

**Other Context Features:**
- **Device:** Mobile vs desktop, OS, screen size
- **Location:** Country, timezone, language
- **Session:** Pages visited, time on site, referrer
- **Placement:** Homepage, category page, search results

### 3.4 Feature Crosses

**What are feature crosses?**
Combinations of features that capture interactions:

```python
# User segment Ã— Time of day
"premium_user_evening" vs "free_user_morning"

# Device Ã— Day of week
"mobile_weekend" vs "desktop_weekday"
```

**When to use:**
- âœ… Tree-based models (LightGBM): Explicit crosses help
- âŒ Deep learning: Learns crosses implicitly (don't need)

---

## 4. Embedding Generation

Embeddings are **dense vector representations** that capture similarity in low-dimensional space. Fundamental to modern recommendation systems.

### 4.1 Embedding Strategies Comparison

| Strategy | When to Use | Pros | Cons | Latency |
|----------|-------------|------|------|---------|
| **Matrix Factorization** | Baseline, cold start | Fast, interpretable | Limited to user-item | 5ms |
| **Two-Tower Neural** | Production systems âœ… | Scalable, cacheable | Needs more data | 8ms |
| **Sequential (Transformer)** | Session-based | Captures temporal | Higher latency | 20ms |
| **Multi-modal** | Rich content | Best accuracy | Most expensive | 30ms |

### 4.2 Two-Tower Architecture (Industry Standard)

**Used by:** YouTube, Google, Meta, Pinterest, TikTok

```
User Features          Item Features
     â”‚                      â”‚
     â†“                      â†“
Dense(256)            Dense(256)
     â”‚                      â”‚
   ReLU                   ReLU
     â”‚                      â”‚
BatchNorm             BatchNorm
     â”‚                      â”‚
Dense(128)            Dense(128)
     â”‚                      â”‚
   ReLU                   ReLU
     â”‚                      â”‚
Dense(128)            Dense(128)
     â”‚                      â”‚
L2 Normalize          L2 Normalize
     â”‚                      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         Dot Product
                â”‚
           Similarity Score
```

**Key Design Decisions:**

1. **Separate Towers**
   - User tower: Encodes user features
   - Item tower: Encodes item features
   - Enables independent caching!

2. **L2 Normalization**
   - Dot product of normalized vectors = cosine similarity
   - Ensures similarity in [-1, 1] range
   - Prevents embedding magnitude dominance

3. **Temperature Scaling**
   - Learnable parameter to adjust similarity sharpness
   - Helps with optimization

**Implementation:**

```python
class TwoTowerModel(nn.Module):
    def __init__(self, user_dim, item_dim, emb_dim=128):
        super().__init__()

        # User tower
        self.user_tower = nn.Sequential(
            nn.Linear(user_dim, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, emb_dim)
        )

        # Item tower (same structure)
        self.item_tower = nn.Sequential(...)

        # Temperature (learnable)
        self.temperature = nn.Parameter(torch.ones(1) * 0.07)

    def forward(self, user_features, item_features):
        user_emb = self.user_tower(user_features)
        item_emb = self.item_tower(item_features)

        # L2 normalize
        user_emb = F.normalize(user_emb, p=2, dim=1)
        item_emb = F.normalize(item_emb, p=2, dim=1)

        # Cosine similarity
        scores = (user_emb * item_emb).sum(dim=1) / self.temperature
        return scores
```

### 4.3 ANN Search with FAISS

**The Challenge:**
- 10 million items Ã— 0.001ms per similarity calculation = **10 seconds** âŒ
- Need sub-second retrieval for real-time serving

**The Solution:** Approximate Nearest Neighbor (ANN) search

```python
import faiss

# Build index
embedding_dim = 128
num_items = 10_000_000

# Option 1: Exact search (baseline)
index = faiss.IndexFlatIP(embedding_dim)  # Inner Product
# Time: O(n) - too slow for large catalogs

# Option 2: IVF (Inverted File) - Production standard
quantizer = faiss.IndexFlatIP(embedding_dim)
index = faiss.IndexIVFFlat(
    quantizer,
    embedding_dim,
    nlist=1000,     # Number of clusters
    faiss.METRIC_INNER_PRODUCT
)
index.train(item_embeddings)
index.add(item_embeddings)
index.nprobe = 10   # Search 10 clusters (1% of total)
# Time: O(k) where k << n - Fast! ~20ms

# Option 3: HNSW (Hierarchical NSW) - Premium
index = faiss.IndexHNSWFlat(embedding_dim, 32)  # M=32
# Time: O(log n) - Fastest, best recall
```

**Index Comparison:**

| Index Type | Search Time | Recall | Memory | Use Case |
|------------|-------------|--------|--------|----------|
| Flat | O(n) = 10s | 100% | 1x | Baseline / < 100K items |
| IVF | O(k) = 20ms | 95%+ | 1.2x | Production standard âœ… |
| HNSW | O(log n) = 10ms | 98%+ | 2x | Premium systems |

**Production Setup:**

```python
# Daily rebuild of FAISS index
def rebuild_item_index():
    # 1. Fetch all items
    item_ids = get_all_item_ids()  # 10M items

    # 2. Generate embeddings (batch on GPU)
    item_embeddings = model.encode_items(item_ids)  # [10M, 128]

    # 3. Build IVF index
    index = faiss.IndexIVFFlat(
        faiss.IndexFlatIP(128),
        128,
        nlist=1000,
        faiss.METRIC_INNER_PRODUCT
    )
    index.train(item_embeddings)
    index.add(item_embeddings)

    # 4. Save to disk
    faiss.write_index(index, "item_index.faiss")
    np.save("item_id_mapping.npy", item_ids)

# Serving: Load index once
index = faiss.read_index("item_index.faiss")
item_id_mapping = np.load("item_id_mapping.npy")

# Search at request time
def search(user_embedding, k=500):
    distances, indices = index.search(
        user_embedding.reshape(1, -1),
        k
    )
    item_ids = [item_id_mapping[idx] for idx in indices[0]]
    return item_ids, distances[0]
```

---

## 5. Two-Stage Retrieval Architecture

**The Critical Question:** Why can't we run complex models on millions of items in real-time?

### 5.1 The Problem

**Naive Approach:**
```
For each user request:
  For each of 10M items:
    - Fetch features (100+ features)
    - Run ranking model
    - Compute score
  Sort by score
  Return top-K
```

**Latency calculation:**
- 10M items Ã— 0.01ms per item = **100 seconds** âŒ
- Completely impractical for real-time serving!

### 5.2 The Solution: Two-Stage Architecture

```
Stage 1: CANDIDATE GENERATION (Fast)
  10,000,000 items â†’ 500 candidates
  Latency: 20ms
  Method: Embedding similarity + ANN search

Stage 2: RANKING (Precise)
  500 candidates â†’ 50 final items
  Latency: 15ms
  Method: Complex ML model (LightGBM/Neural)

Total: 35ms âœ…
```

### Stage 1: Candidate Generation (Details)

**Goal:** Quickly narrow down millions â†’ hundreds

**Flow:**

```python
def candidate_generation(user_id: str, num_candidates: int = 500):
    """
    Stage 1: Fast retrieval using embeddings

    Latency budget: 20-30ms
    """
    # 1. Get user embedding (5ms)
    user_emb = get_user_embedding(user_id)
    # Try cache first (Redis)
    # If miss: fetch features + encode

    # 2. Normalize (< 1ms)
    user_emb = user_emb / np.linalg.norm(user_emb)

    # 3. ANN search (15-20ms)
    distances, indices = faiss_index.search(
        user_emb.reshape(1, -1),
        num_candidates * 3  # Over-retrieve for filtering
    )

    # 4. Apply filters (2-3ms)
    # - In stock
    # - Region allowed
    # - Not recently shown

    # 5. Return top candidates
    candidate_items = [item_id_mapping[idx] for idx in indices[0]]
    return candidate_items[:num_candidates]
```

**Caching Strategy:**

```python
# User embeddings cached in Redis
cache_key = f"user_emb:{user_id}"
cached_emb = redis.get(cache_key)

if cached_emb:
    # Cache hit (99% of requests)
    user_emb = np.frombuffer(cached_emb, dtype=np.float32)
else:
    # Cache miss - compute and cache
    user_features = fetch_user_features(user_id)
    user_emb = user_encoder(user_features)
    redis.setex(cache_key, ttl=3600, value=user_emb.tobytes())
```

### Stage 2: Ranking (Details)

**Goal:** Accurately score candidates with complex model

**Flow:**

```python
def ranking(user_id: str, candidate_items: List[str]):
    """
    Stage 2: Precise scoring

    Latency budget: 15-30ms
    """
    # 1. Fetch features in parallel (10ms)
    with ThreadPoolExecutor(max_workers=4) as executor:
        user_features = executor.submit(get_user_features, user_id)
        item_features = executor.submit(get_item_features, candidate_items)
        context_features = executor.submit(get_context_features)

        # Wait for all
        user_feat = user_features.result()
        item_feat = item_features.result()
        ctx_feat = context_features.result()

    # 2. Create feature vectors (2ms)
    feature_matrix = create_ranking_features(
        user_feat, item_feat, ctx_feat
    )  # Shape: [500, 100+]

    # 3. Model inference (10ms)
    scores = lightgbm_model.predict(feature_matrix)

    # 4. Sort by score (< 1ms)
    sorted_indices = np.argsort(-scores)
    ranked_items = [candidate_items[i] for i in sorted_indices]

    return ranked_items, scores
```

**Feature Fetching (Critical Path Optimization):**

```python
# BAD: Sequential fetching (30ms)
user_features = get_user_features(user_id)        # 10ms
item_features = get_item_features(items)          # 10ms
context_features = get_context_features()         # 10ms

# GOOD: Parallel fetching (10ms)
with ThreadPoolExecutor() as executor:
    futures = [
        executor.submit(get_user_features, user_id),
        executor.submit(get_item_features, items),
        executor.submit(get_context_features)
    ]
    user_feat, item_feat, ctx_feat = [f.result() for f in futures]
```

### 5.3 Model Choice: LightGBM vs Neural Networks

**LightGBM (Our Production Choice):**

```python
import lightgbm as lgb

# Training
params = {
    'objective': 'lambdarank',  # Optimize NDCG directly
    'metric': 'ndcg',
    'num_leaves': 64,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'max_depth': 8
}

model = lgb.train(
    params,
    train_data,
    num_boost_round=1000,
    valid_sets=[val_data],
    callbacks=[lgb.early_stopping(50)]
)

# Serving
scores = model.predict(features)  # 10ms for 500 items
```

**Why LightGBM?**
- âœ… **Fast:** 10ms for 500 items
- âœ… **Handles mixed types:** Numeric + categorical naturally
- âœ… **Feature interactions:** Built-in
- âœ… **Interpretable:** Feature importance
- âœ… **Production-proven:** Used at scale

**Deep Learning Alternative:**

```python
class DeepCrossNetwork(nn.Module):
    """
    DCN: Deep & Cross Network
    - Cross layers: Explicit feature interactions
    - Deep layers: Implicit interactions
    """
    # 20-30ms for 500 items (GPU batching)
    # Better for very large datasets (> 1B samples)
    # More complex to deploy and maintain
```

**Our Hybrid Approach:**
- Use transformers for **embeddings** (Stage 1)
- Use LightGBM for **ranking** (Stage 2)
- Best of both worlds!

---

## 6. Model Training & Evaluation

### 6.1 Daily Retraining Pipeline

```
00:00 - 02:00 UTC: Data Collection
â”œâ”€ Aggregate last 7 days of interactions
â”œâ”€ Join user profiles and item metadata
â”œâ”€ Run validation and quality checks
â””â”€ Output: Clean training data

02:00 - 04:00 UTC: Feature Engineering
â”œâ”€ Compute user statistics
â”œâ”€ Generate item popularity metrics
â”œâ”€ Create sequential features
â”œâ”€ Build embedding vocabularies
â””â”€ Output: Feature-rich dataset

04:00 - 10:00 UTC: Model Training
â”œâ”€ Embedding models: 2-4 hours on 4 GPUs
â”œâ”€ Ranking models: 1-2 hours on 16 CPUs
â”œâ”€ Hyperparameter tuning (Optuna)
â”œâ”€ Cross-validation
â””â”€ Output: Trained models

10:00 - 11:00 UTC: Evaluation
â”œâ”€ Offline metrics: AUC, NDCG, Log Loss
â”œâ”€ Compare with baseline
â”œâ”€ Generate evaluation report
â””â”€ Output: Model quality assessment

11:00 - 12:00 UTC: Deployment
â”œâ”€ A/B test on 5% traffic
â”œâ”€ Monitor online metrics
â”œâ”€ Gradual rollout if successful
â””â”€ Output: New model in production
```

### 6.2 Evaluation Metrics

**Offline Metrics (Test Set):**

| Metric | Purpose | Target | Why It Matters |
|--------|---------|--------|----------------|
| **AUC-ROC** | Binary classification quality | > 0.75 | Overall model discriminative power |
| **Log Loss** | Probability calibration | < 0.35 | Predicted probabilities accuracy |
| **NDCG@10** | Ranking quality | > 0.80 | Position matters in recommendations |
| **MAP@10** | Precision at top | > 0.60 | Relevant items at top positions |
| **Coverage** | Catalog diversity | > 30% | Don't ignore long tail |
| **Novelty** | Surprise factor | Balanced | Avoid filter bubbles |

**Online Metrics (Production):**

| Metric | Current | Target | Measurement |
|--------|---------|--------|-------------|
| **CTR** | 4.2% | > 3% | Clicks / Impressions |
| **CVR** | 1.8% | > 1% | Purchases / Clicks |
| **Engagement** | 12.5% | > 10% | Any interaction / Impressions |
| **Revenue** | $12.50 eCPM | > $10 | Revenue per 1000 impressions |
| **Latency p99** | 48ms | < 100ms | 99th percentile response time |

### 6.3 The Critical Gap: Offline â‰  Online

```
Offline Metrics          Online Metrics
(Test Set)              (Real Users)

AUC = 0.80        â‰      CTR = 3.5%
NDCG = 0.82       â‰      Engagement = 12%
Log Loss = 0.32   â‰      Revenue = $12 eCPM
```

**Why the gap?**

1. **Distribution Shift**
   - Training data is historical (old)
   - User behavior evolves
   - Item catalog changes

2. **Position Bias**
   - Users click top results more
   - Test set assumes uniform presentation
   - Reality: position matters tremendously

3. **Selection Bias**
   - Test set only has shown items
   - Production: all items are candidates
   - Exposure bias affects metrics

4. **Interaction Effects**
   - Users influence each other
   - Temporal patterns matter
   - Context changes behavior

**Solution:** Always A/B test before full deployment!

```python
# A/B Test Framework
def ab_test_new_model(new_model, control_model):
    """
    Test new model on 5% traffic for 7 days
    """
    for request in incoming_requests:
        # Random assignment
        if hash(request.user_id) % 100 < 5:
            # Treatment: new model
            response = new_model.recommend(request)
            log_experiment(user_id, variant="treatment")
        else:
            # Control: existing model
            response = control_model.recommend(request)
            log_experiment(user_id, variant="control")

    # After 7 days: analyze results
    results = analyze_ab_test(
        control_conversions, control_samples,
        treatment_conversions, treatment_samples
    )

    if results['p_value'] < 0.05 and results['lift'] > 0.02:
        deploy_to_production(new_model)
```

---

## 7. Production Serving

### 7.1 Infrastructure Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Load Balancer (Nginx)                 â”‚
â”‚                    Distributes traffic                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server 1   â”‚    â”‚  API Server N   â”‚
â”‚    (FastAPI)    â”‚... â”‚    (FastAPI)    â”‚
â”‚  - Request      â”‚    â”‚  - Request      â”‚
â”‚  - Validation   â”‚    â”‚  - Validation   â”‚
â”‚  - Orchestrationâ”‚    â”‚  - Orchestrationâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feature Store    â”‚          â”‚  Model Serving        â”‚
â”‚  - Feast (Redis)  â”‚          â”‚  - NVIDIA Triton      â”‚
â”‚  - Online featuresâ”‚          â”‚  - GPU batching       â”‚
â”‚  - <10ms latency  â”‚          â”‚  - Model versioning   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                   â”‚
         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cache (Redis)       â”‚      â”‚  ANN Search (FAISS)    â”‚
â”‚  - User embeddings   â”‚      â”‚  - Item index (GPU)    â”‚
â”‚  - Popular items     â”‚      â”‚  - 10M items           â”‚
â”‚  - Feature vectors   â”‚      â”‚  - 20ms retrieval      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Deployment Details:**

```yaml
# Kubernetes Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation-service
spec:
  replicas: 100  # Horizontal scaling
  template:
    spec:
      containers:
      - name: api-server
        image: recommendation-service:v1.2.3
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        env:
        - name: MODEL_VERSION
          value: "v1.2.3"
        - name: REDIS_HOST
          value: "redis-cluster"
        - name: FAISS_INDEX_PATH
          value: "s3://models/faiss/index.faiss"
```

**Auto-scaling:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: recommendation-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: recommendation-service
  minReplicas: 50
  maxReplicas: 200
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: request_latency_p99
      target:
        type: AverageValue
        averageValue: "100"  # milliseconds
```

### 7.2 Caching Strategy

**Multi-layer caching for optimal latency:**

| Cache Type | Data | TTL | Benefit | Hit Rate |
|------------|------|-----|---------|----------|
| **User Embeddings** | 128-dim vectors | 1 hour | Skip feature fetch + encoding (15ms saved) | 99% |
| **Popular Items** | Top 1000 items | 15 min | Cold start fallback | 5% |
| **Item Metadata** | Category, price, etc. | 1 day | Reduce DB queries | 95% |
| **Feature Vectors** | Pre-computed features | 6 hours | Faster ranking | 60% |

**Implementation:**

```python
import redis
import numpy as np

redis_client = redis.Redis(host='localhost', port=6379)

def get_user_embedding(user_id: str) -> np.ndarray:
    """
    Multi-tier caching strategy
    """
    # L1: Redis cache (hot path)
    cache_key = f"user_emb:{user_id}"
    cached = redis_client.get(cache_key)

    if cached:
        # Cache hit: 99% of requests (< 1ms)
        return np.frombuffer(cached, dtype=np.float32)

    # L2: Compute from features (cold path)
    user_features = fetch_user_features(user_id)  # 5ms
    user_emb = user_encoder(user_features)         # 5ms

    # Cache for 1 hour
    redis_client.setex(
        cache_key,
        3600,  # TTL in seconds
        user_emb.tobytes()
    )

    return user_emb

# Cache warming for popular users
def warm_cache():
    """
    Pre-compute embeddings for top 10K users
    Run every hour
    """
    popular_users = get_popular_users(limit=10000)

    for user_id in popular_users:
        user_emb = compute_user_embedding(user_id)
        cache_key = f"user_emb:{user_id}"
        redis_client.setex(cache_key, 3600, user_emb.tobytes())
```

### 7.3 Business Logic Layer

Post-processing rules applied after model scoring:

```python
class BusinessLogicLayer:
    """
    Apply business rules to model outputs
    """

    def apply_rules(self, items: List[str], scores: List[float],
                    user_id: str) -> Tuple[List[str], List[float]]:
        """
        Multi-step post-processing
        """
        # 1. Deduplication (remove recently shown)
        items, scores = self.deduplicate(user_id, items, scores)

        # 2. Diversity (max N per category)
        items, scores = self.enforce_diversity(items, scores)

        # 3. Freshness boost (new content)
        scores = self.apply_freshness_boost(items, scores)

        # 4. Business filters (safety, stock, region)
        items, scores = self.apply_filters(items, scores)

        # 5. Re-rank
        sorted_indices = np.argsort(-np.array(scores))
        items = [items[i] for i in sorted_indices]
        scores = [scores[i] for i in sorted_indices]

        return items, scores

    def enforce_diversity(self, items, scores):
        """
        Ensure variety: max 3 items per category in top-10

        Prevents over-concentration on popular categories
        Improves user experience through variety
        """
        category_counts = {}
        diverse_items, diverse_scores = [], []

        for item, score in zip(items, scores):
            category = get_item_category(item)
            count = category_counts.get(category, 0)

            if count < 3:  # Max 3 per category
                diverse_items.append(item)
                diverse_scores.append(score)
                category_counts[category] = count + 1

        return diverse_items, diverse_scores

    def apply_freshness_boost(self, items, scores):
        """
        Boost recently added items

        Formula: score Ã— (1 + 0.1 Ã— e^(-age/30))
        - New items (age=0): 10% boost
        - 30-day old items: 3.7% boost
        - 90-day old items: 0.5% boost
        """
        boosted_scores = []

        for item, score in zip(items, scores):
            age_days = get_item_age_days(item)
            boost = 0.1 * np.exp(-age_days / 30)
            boosted_score = score * (1 + boost)
            boosted_scores.append(boosted_score)

        return boosted_scores
```

---

## 8. Monitoring & Observability

### 8.1 Key Metrics Dashboard

**System Health Metrics:**

```python
# Prometheus metrics
from prometheus_client import Counter, Histogram, Gauge

# Request metrics
request_count = Counter(
    'recommendation_requests_total',
    'Total recommendation requests',
    ['endpoint', 'status']
)

request_latency = Histogram(
    'recommendation_latency_seconds',
    'Request latency',
    ['stage']  # candidate_gen, ranking, business_logic
)

# Model metrics
model_predictions = Counter(
    'model_predictions_total',
    'Total predictions',
    ['model_version']
)

# Business metrics
ctr = Gauge('recommendation_ctr', 'Click-through rate')
conversion_rate = Gauge('recommendation_cvr', 'Conversion rate')
revenue = Gauge('recommendation_revenue_usd', 'Revenue generated')
```

**Monitoring Thresholds:**

| Category | Metric | Alert Threshold | Action |
|----------|--------|----------------|--------|
| **Latency** | p99 | > 100ms | Scale up replicas |
| **Throughput** | QPS | Drop > 20% | Check upstream |
| **Model Quality** | CTR | Drop > 5% | Investigate drift |
| **Data Drift** | PSI | > 0.2 | Retrain model |
| **System Health** | CPU | > 80% | Scale resources |
| **Error Rate** | 5xx | > 0.1% | Rollback if needed |

### 8.2 Data Drift Detection

**Population Stability Index (PSI):** Industry standard

```python
def compute_psi(baseline: np.ndarray, current: np.ndarray, bins: int = 10) -> float:
    """
    PSI = Î£ (current% - baseline%) Ã— ln(current% / baseline%)

    Interpretation:
    - PSI < 0.1: No significant drift âœ“
    - 0.1 < PSI < 0.2: Moderate drift âš 
    - PSI > 0.2: Significant drift - retrain! ğŸ”´
    """
    # Create bins from baseline
    breakpoints = np.percentile(baseline, np.linspace(0, 100, bins + 1))
    breakpoints = np.unique(breakpoints)

    # Count samples in each bin
    baseline_counts = np.histogram(baseline, bins=breakpoints)[0]
    current_counts = np.histogram(current, bins=breakpoints)[0]

    # Convert to percentages
    baseline_pct = baseline_counts / len(baseline)
    current_pct = current_counts / len(current)

    # Avoid division by zero
    baseline_pct = np.where(baseline_pct == 0, 0.0001, baseline_pct)
    current_pct = np.where(current_pct == 0, 0.0001, current_pct)

    # Calculate PSI
    psi = np.sum((current_pct - baseline_pct) * np.log(current_pct / baseline_pct))

    return psi

# Monitor all features
features_to_monitor = [
    'user_age', 'user_recency', 'user_ctr',
    'item_price', 'item_popularity', 'item_ctr'
]

for feature in features_to_monitor:
    baseline = train_data[feature]
    current = production_data[feature]
    psi = compute_psi(baseline, current)

    if psi > 0.2:
        alert(f"Significant drift detected in {feature}: PSI = {psi:.3f}")
        trigger_retraining()
```

### 8.3 A/B Testing Framework

**Statistical rigor in experimentation:**

```python
def calculate_required_sample_size(
    baseline_ctr: float = 0.03,
    minimum_detectable_effect: float = 0.05,  # 5% lift
    alpha: float = 0.05,  # Significance level
    power: float = 0.80   # Statistical power
) -> int:
    """
    Calculate required sample size per variant

    Example:
    - Baseline CTR: 3%
    - Want to detect: 5% relative lift (3% â†’ 3.15%)
    - Significance: p < 0.05
    - Power: 80%

    Result: ~100K users per variant needed
    """
    from statsmodels.stats.power import zt_ind_solve_power

    p1 = baseline_ctr
    p2 = baseline_ctr * (1 + minimum_detectable_effect)

    # Effect size (Cohen's h for proportions)
    effect_size = 2 * (np.arcsin(np.sqrt(p2)) - np.arcsin(np.sqrt(p1)))

    # Sample size per variant
    n = zt_ind_solve_power(
        effect_size=effect_size,
        alpha=alpha,
        power=power,
        alternative='two-sided'
    )

    return int(np.ceil(n))

def analyze_ab_test(
    control_conversions: int,
    control_samples: int,
    treatment_conversions: int,
    treatment_samples: int
) -> Dict:
    """
    Statistical analysis of A/B test results
    """
    control_rate = control_conversions / control_samples
    treatment_rate = treatment_conversions / treatment_samples

    # Relative lift
    lift = (treatment_rate - control_rate) / control_rate

    # Z-test for proportions
    pooled_rate = (control_conversions + treatment_conversions) / \
                  (control_samples + treatment_samples)

    se = np.sqrt(pooled_rate * (1 - pooled_rate) *
                 (1/control_samples + 1/treatment_samples))
    z_score = (treatment_rate - control_rate) / se

    # Two-tailed p-value
    from scipy import stats
    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))

    # 95% Confidence interval
    se_diff = np.sqrt(
        control_rate * (1 - control_rate) / control_samples +
        treatment_rate * (1 - treatment_rate) / treatment_samples
    )
    ci_lower = (treatment_rate - control_rate) - 1.96 * se_diff
    ci_upper = (treatment_rate - control_rate) + 1.96 * se_diff

    # Decision
    is_significant = p_value < 0.05
    is_positive_lift = lift > 0.02  # Minimum 2% lift
    should_ship = is_significant and is_positive_lift

    return {
        'control_rate': f"{control_rate:.3%}",
        'treatment_rate': f"{treatment_rate:.3%}",
        'relative_lift': f"{lift:.2%}",
        'p_value': f"{p_value:.4f}",
        'is_significant': is_significant,
        'confidence_interval_95': (f"{ci_lower:.3%}", f"{ci_upper:.3%}"),
        'recommendation': 'Ship âœ…' if should_ship else 'Do not ship âŒ',
        'reasoning': (
            f"{'Significant' if is_significant else 'Not significant'} result "
            f"with {lift:.1%} lift. "
            f"{'Meets' if is_positive_lift else 'Does not meet'} minimum threshold."
        )
    }

# Example usage
results = analyze_ab_test(
    control_conversions=3000,
    control_samples=100000,
    treatment_conversions=3300,
    treatment_samples=100000
)

print(results)
# {
#     'control_rate': '3.000%',
#     'treatment_rate': '3.300%',
#     'relative_lift': '10.00%',
#     'p_value': '0.0012',
#     'is_significant': True,
#     'confidence_interval_95': ('0.120%', '0.480%'),
#     'recommendation': 'Ship âœ…',
#     'reasoning': 'Significant result with 10.0% lift. Meets minimum threshold.'
# }
```

---

## 9. End-to-End Request Flow

**Complete flow from API request to response (57ms total):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIME: 0ms                                                    â”‚
â”‚ STEP 1: Request Received                                    â”‚
â”‚                                                              â”‚
â”‚ GET /recommend?user_id=12345&num_items=20                   â”‚
â”‚ â†’ Load balancer routes to available replica                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIME: 0-5ms (5ms elapsed)                                   â”‚
â”‚ STEP 2: User Embedding Fetch                                â”‚
â”‚                                                              â”‚
â”‚ 1. Check Redis: user_emb:12345                              â”‚
â”‚    â†’ Cache hit (99% of requests): 1ms âœ…                    â”‚
â”‚    â†’ Cache miss (1% of requests): compute + cache (10ms)    â”‚
â”‚                                                              â”‚
â”‚ 2. If miss:                                                  â”‚
â”‚    â†’ Fetch features from Feast (5ms)                        â”‚
â”‚    â†’ Encode with user tower (3ms)                           â”‚
â”‚    â†’ Cache in Redis (1ms)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIME: 5-25ms (20ms elapsed)                                 â”‚
â”‚ STEP 3: Candidate Generation                                â”‚
â”‚                                                              â”‚
â”‚ 1. Normalize user embedding (L2 norm): <1ms                 â”‚
â”‚                                                              â”‚
â”‚ 2. FAISS ANN search on 10M items:                           â”‚
â”‚    index.search(user_emb, k=500)                            â”‚
â”‚    â†’ IVF search with nprobe=10: ~18ms                       â”‚
â”‚    â†’ Returns 500 item indices + similarity scores           â”‚
â”‚                                                              â”‚
â”‚ 3. Map indices to item IDs: <1ms                            â”‚
â”‚                                                              â”‚
â”‚ 4. Apply basic filters: 1ms                                 â”‚
â”‚    â†’ In stock: check inventory                              â”‚
â”‚    â†’ Region allowed: check geo-restrictions                 â”‚
â”‚    â†’ Not recently shown: check Redis dedup list             â”‚
â”‚                                                              â”‚
â”‚ Output: 500 candidate item IDs                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIME: 25-35ms (10ms elapsed)                                â”‚
â”‚ STEP 4: Feature Fetching (PARALLEL)                         â”‚
â”‚                                                              â”‚
â”‚ ThreadPoolExecutor with 3 workers:                          â”‚
â”‚                                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Thread 1:       â”‚  â”‚ Thread 2:       â”‚  â”‚ Thread 3:    â”‚ â”‚
â”‚ â”‚                 â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚ â”‚ User Features   â”‚  â”‚ Item Features   â”‚  â”‚ Context Feat â”‚ â”‚
â”‚ â”‚ - Demographics  â”‚  â”‚ - MetadataÃ—500  â”‚  â”‚ - Time       â”‚ â”‚
â”‚ â”‚ - Behavior      â”‚  â”‚ - Popularity    â”‚  â”‚ - Device     â”‚ â”‚
â”‚ â”‚ - Preferences   â”‚  â”‚ - Quality       â”‚  â”‚ - Location   â”‚ â”‚
â”‚ â”‚                 â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚ â”‚ Feast API: 8ms  â”‚  â”‚ Redis: 6ms      â”‚  â”‚ Compute: 2ms â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚ Total: max(8, 6, 2) = 8ms (parallel) + 2ms (join) = 10ms   â”‚
â”‚                                                              â”‚
â”‚ Output: Feature matrix [500, 100+]                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIME: 35-50ms (15ms elapsed)                                â”‚
â”‚ STEP 5: Ranking Model Inference                             â”‚
â”‚                                                              â”‚
â”‚ 1. LightGBM batch prediction:                               â”‚
â”‚    scores = model.predict(feature_matrix)                   â”‚
â”‚    â†’ 500 items, 100 features                                â”‚
â”‚    â†’ CPU inference: 12ms                                    â”‚
â”‚                                                              â”‚
â”‚ 2. Sort by score:                                           â”‚
â”‚    sorted_indices = np.argsort(-scores)                     â”‚
â”‚    â†’ NumPy sort: <1ms                                       â”‚
â”‚                                                              â”‚
â”‚ 3. Reorder items and scores: <1ms                           â”‚
â”‚                                                              â”‚
â”‚ Output: Ranked 500 items with scores                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIME: 50-55ms (5ms elapsed)                                 â”‚
â”‚ STEP 6: Business Logic                                      â”‚
â”‚                                                              â”‚
â”‚ 1. Diversity enforcement: 2ms                               â”‚
â”‚    â†’ Max 3 items per category in top-10                     â”‚
â”‚    â†’ Sliding window approach                                â”‚
â”‚                                                              â”‚
â”‚ 2. Freshness boost: 1ms                                     â”‚
â”‚    â†’ score Ã— (1 + 0.1 Ã— e^(-age/30))                        â”‚
â”‚    â†’ Boosts new items                                       â”‚
â”‚                                                              â”‚
â”‚ 3. Deduplication: 1ms                                       â”‚
â”‚    â†’ Check Redis: recent_items:user_id                      â”‚
â”‚    â†’ Remove recently shown items                            â”‚
â”‚                                                              â”‚
â”‚ 4. Re-rank after adjustments: <1ms                          â”‚
â”‚                                                              â”‚
â”‚ Output: Final ranked list                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIME: 55-57ms (2ms elapsed)                                 â”‚
â”‚ STEP 7: Response Construction                               â”‚
â”‚                                                              â”‚
â”‚ 1. Take top 20 items: <1ms                                  â”‚
â”‚                                                              â”‚
â”‚ 2. Fetch display metadata (cached): 1ms                     â”‚
â”‚    â†’ Title, image URL, price, rating                        â”‚
â”‚    â†’ From Redis metadata cache                              â”‚
â”‚                                                              â”‚
â”‚ 3. Format JSON response: <1ms                               â”‚
â”‚    {                                                         â”‚
â”‚      "user_id": "12345",                                     â”‚
â”‚      "items": [                                              â”‚
â”‚        {"item_id": "...", "score": 0.95, "rank": 1, ...},  â”‚
â”‚        ...                                                   â”‚
â”‚      ],                                                      â”‚
â”‚      "latency_ms": 57,                                       â”‚
â”‚      "model_version": "v1.2.3"                              â”‚
â”‚    }                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIME: 57ms TOTAL                                            â”‚
â”‚ STEP 8: Response Sent + Logging                             â”‚
â”‚                                                              â”‚
â”‚ 1. Return JSON to client                                    â”‚
â”‚                                                              â”‚
â”‚ 2. Async logging (non-blocking):                            â”‚
â”‚    â†’ Request details (user, items shown, scores)            â”‚
â”‚    â†’ Latency breakdown by stage                             â”‚
â”‚    â†’ Model version and experiment ID                        â”‚
â”‚    â†’ Used for offline learning and A/B analysis             â”‚
â”‚                                                              â”‚
â”‚ âœ… Well under 100ms p99 SLA                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Latency Breakdown Table

| Stage | Latency | Critical Path? | Optimization |
|-------|---------|----------------|--------------|
| User Embedding Fetch | 5ms | âœ… Yes | Redis caching (99% hit rate) |
| Candidate Generation | 20ms | âœ… Yes | FAISS GPU, IVF indexing |
| Feature Fetching | 10ms | âœ… Yes | Parallel ThreadPool, Feast |
| Model Ranking | 15ms | âœ… Yes | LightGBM batching, CPU |
| Business Logic | 5ms | No | In-memory processing |
| Response Format | 2ms | No | JSON serialization |
| **TOTAL** | **57ms** | - | **p99 < 100ms SLA âœ…** |

---

## 10. Key Design Decisions

### 10.1 Why Two-Stage Architecture?

**The Problem:**

```python
# Naive approach: Score all items with complex model
for each user request:
    for each of 10,000,000 items:
        features = fetch_features(user, item)  # 0.001ms
        score = complex_model.predict(features)  # 0.01ms

    sort_by_score()
    return top_K

# Latency: 10M Ã— 0.011ms = 110,000ms = 110 seconds âŒ
```

**Our Solution:**

```python
# Two-stage approach
for each user request:
    # Stage 1: Fast filtering
    candidates = embedding_search(user, items=10M, k=500)  # 20ms

    # Stage 2: Precise ranking
    for each of 500 candidates:
        features = fetch_features(user, item)
        score = complex_model.predict(features)

    sort_by_score()
    return top_K

# Latency: 20ms + (500 Ã— 0.01ms) = 25ms âœ…
```

**Trade-offs:**

| Aspect | Naive | Two-Stage |
|--------|-------|-----------|
| **Latency** | 110s âŒ | 25ms âœ… |
| **Accuracy** | Best possible | Slightly lower (ANN) |
| **Recall@500** | 100% | 95%+ |
| **Scalability** | O(n) | O(log n) |
| **Production Viable** | No | Yes |

**Key Insight:** We trade 5% recall for 4000x speedup!

### 10.2 LightGBM vs Deep Learning?

**When to use what:**

```
Use LightGBM when:
âœ… Tabular features dominate (user stats, item metadata)
âœ… Need fast inference (< 10ms for 500 items)
âœ… Want interpretability (feature importance)
âœ… Have moderate data (< 1B samples)
âœ… Production constraints (latency, resources)

Use Deep Learning when:
âœ… Unstructured data (text, images, audio)
âœ… Massive data (> 1B samples)
âœ… Complex interactions critical
âœ… Can afford higher latency (20-50ms)
âœ… Transfer learning benefits
```

**Our Hybrid Approach:**

```python
# Best of both worlds

# Stage 1: Use transformers for embeddings
user_emb = bert_model.encode(user_text)
item_emb = resnet_model.encode(item_image)
# â†’ Captures semantic similarity

# Stage 2: Use LightGBM for ranking
features = [user_stats, item_stats, embeddings, cross_features]
score = lightgbm_model.predict(features)
# â†’ Fast, accurate, interpretable
```

### 10.3 Why Feature Store is Essential

**The Problem: Training/Serving Skew**

```python
# WITHOUT Feature Store

# Training (Python, Spark SQL on S3)
def compute_user_ctr_training(user_id):
    query = f"""
        SELECT COUNT(DISTINCT CASE WHEN event='click' THEN 1 END) /
               COUNT(*) as ctr
        FROM interactions
        WHERE user_id = '{user_id}'
    """
    return spark.sql(query)

# Serving (Python, different code on PostgreSQL)
def compute_user_ctr_serving(user_id):
    clicks = db.query(f"SELECT COUNT(*) FROM clicks WHERE user={user_id}")
    views = db.query(f"SELECT COUNT(*) FROM views WHERE user={user_id}")
    return clicks / views if views > 0 else 0

# âŒ Different logic â†’ Different results â†’ Poor production performance
```

**With Feature Store (Feast):**

```python
# Single source of truth

# Feature definition (once)
from feast import FeatureView, Field
from feast.types import Float32

user_ctr = FeatureView(
    name="user_ctr",
    entities=["user"],
    schema=[Field(name="ctr", dtype=Float32)],
    source=interaction_data,  # Data source
    online=True,  # Enable online serving
    offline=True  # Enable offline training
)

# Training (offline)
features = feast_store.get_historical_features(
    entity_df=training_data,
    features=["user_ctr:ctr"]
)

# Serving (online)
features = feast_store.get_online_features(
    entity_rows=[{"user_id": 12345}],
    features=["user_ctr:ctr"]
)

# âœ… Same definition â†’ Same computation â†’ Consistent results
```

**Impact:**
- Eliminates training/serving skew
- Typical 5-10% accuracy improvement
- Point-in-time correctness (no data leakage)
- Feature versioning and lineage

### 10.4 Why Daily Retraining?

**User behavior changes constantly:**

| Timeframe | Changes | Impact |
|-----------|---------|--------|
| **Daily** | New items, trending topics | Need fresh embeddings |
| **Weekly** | User preference shifts | Need updated user models |
| **Monthly** | Seasonal patterns | Need adaptive weights |
| **Quarterly** | Market dynamics | Need architecture updates |

**Retraining Frequency Trade-offs:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frequency   â”‚ Freshnessâ”‚ Compute Costâ”‚ Recommended  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Real-time   â”‚ Perfect  â”‚ Very High   â”‚ Rarely       â”‚
â”‚ Hourly      â”‚ Excellentâ”‚ High        â”‚ Large catalogsâ”‚
â”‚ Daily       â”‚ Good     â”‚ Moderate    â”‚ Most systems âœ…â”‚
â”‚ Weekly      â”‚ Fair     â”‚ Low         â”‚ Small catalogsâ”‚
â”‚ Monthly     â”‚ Stale    â”‚ Very Low    â”‚ Not advised  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Our Approach:**

```python
# Daily batch retraining
schedule = "0 4 * * *"  # 4 AM UTC daily

def daily_retraining():
    # 1. Data collection (2 hours)
    data = collect_last_7_days()

    # 2. Feature engineering (2 hours)
    features = compute_features(data)

    # 3. Model training (4 hours)
    model = train_model(features)

    # 4. Evaluation (1 hour)
    metrics = evaluate_model(model)

    # 5. Deployment (1 hour)
    if metrics['auc'] > baseline['auc'] + 0.01:
        deploy_model(model, traffic_percent=5)  # A/B test

# Plus: Hourly embedding updates for new items
schedule_hourly = "0 * * * *"

def hourly_embedding_update():
    new_items = get_items_added_last_hour()
    item_embeddings = model.encode_items(new_items)
    faiss_index.add(item_embeddings)
```

---

## Appendix: Quick Reference

### System Components

| Component | Technology | Purpose | Latency |
|-----------|------------|---------|---------|
| **Data Pipeline** | PySpark | ETL and validation | Batch |
| **Feature Store** | Feast + Redis + S3 | Online/offline features | < 10ms |
| **Embedding Models** | PyTorch Two-Tower | User/item vectors | 8ms |
| **ANN Search** | FAISS (IVF) | Fast candidate retrieval | 20ms |
| **Ranking Model** | LightGBM | Precise scoring | 15ms |
| **API Server** | FastAPI | REST endpoints | < 5ms |
| **Caching** | Redis | Embeddings, metadata | < 1ms |
| **Orchestration** | Kubernetes | Scaling, deployment | - |
| **Monitoring** | Prometheus + Grafana | Metrics, alerts | - |
| **Experiments** | Custom A/B | Statistical testing | - |
| **ML Tracking** | MLflow | Model versioning | - |

### Performance Targets

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| **Latency p99** | < 100ms | 48ms | âœ… |
| **Throughput** | > 10K QPS | 12K QPS | âœ… |
| **CTR** | > 3% | 4.2% | âœ… |
| **CVR** | > 1% | 1.8% | âœ… |
| **Model AUC** | > 0.75 | 0.78 | âœ… |
| **NDCG@10** | > 0.80 | 0.82 | âœ… |
| **Uptime** | > 99.9% | 99.95% | âœ… |

### Key Files in Codebase

```
recommendation_system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_pipeline/
â”‚   â”‚   â””â”€â”€ data_loader.py              # Data ingestion & validation
â”‚   â”œâ”€â”€ feature_engineering/
â”‚   â”‚   â””â”€â”€ feature_pipeline.py         # Feature computation
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ embedding_models.py         # Two-Tower, MF, Sequential
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ ranking_model.py            # LightGBM, DeepFM, DCN
â”‚   â”œâ”€â”€ serving/
â”‚   â”‚   â””â”€â”€ recommendation_service.py   # FastAPI production server
â”‚   â””â”€â”€ monitoring/
â”‚       â””â”€â”€ monitoring.py               # Drift detection, A/B tests
â””â”€â”€ demo_pipeline.py                    # End-to-end demo
```

---

## Summary

This recommendation system demonstrates **production-grade ML engineering** at scale:

âœ… **Scalable:** Handles billions of requests with sub-100ms latency
âœ… **Accurate:** Transformer embeddings + LightGBM ranking
âœ… **Reliable:** Feature stores prevent training/serving skew
âœ… **Observable:** Comprehensive monitoring and drift detection
âœ… **Tested:** Rigorous A/B testing before deployment
âœ… **Maintainable:** Clean architecture, daily retraining

**Key Takeaways:**

1. **Two-stage architecture** is essential for real-time serving at scale
2. **Feature stores** (Feast) eliminate training/serving skew
3. **Embeddings + ANN** enable sub-linear retrieval
4. **LightGBM** is the production workhorse for ranking
5. **Caching** is critical for meeting latency SLAs
6. **Monitoring drift** triggers retraining automatically
7. **A/B testing** validates offline improvements online

---

*This documentation covers the complete flow of a production-scale recommendation system designed for platforms serving billions of users. The architecture emphasizes scalability, low latency, and maintainability while balancing user experience with business objectives.*
